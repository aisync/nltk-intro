{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW Dictionaries\n",
    "One of the most common ways to implement the BoW model in Python is as a **dictionary with each key set to a word and each value set to the number of times that word appears.**\n",
    "\n",
    "The words from the sentence go into the bag-of-words and come out as a dictionary of words with their corresponding counts. For statistical models, we call the text that we use to build the model our training data. Usually, we need to prepare our text data by breaking it up into documents (shorter strings of text, generally sentences).\n",
    "\n",
    "**Let’s build a function that converts a given training text into a bag-of-words!**\n",
    "```python\n",
    "from preprocessing import preprocess_text\n",
    "# Define text_to_bow() below:\n",
    "def text_to_bow(some_text):\n",
    "  bow_dictionary = {}\n",
    "  tokens = preprocess_text(some_text)\n",
    "  for token in tokens:\n",
    "    if token in bow_dictionary:\n",
    "      bow_dictionary[token] += 1\n",
    "    else:\n",
    "      bow_dictionary[token] = 1\n",
    "\n",
    "  return bow_dictionary\n",
    "\n",
    "print(text_to_bow(\"I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish...\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bow Vectors\n",
    "A feature vector is a numeric representation of an item’s important features. Each feature has its own column. If the feature exists for the item, you could represent that with a 1. If the feature does not exist for that item, you could represent that with a `0`. \n",
    "\n",
    "Turning text into a BoW vector is known as **feature extraction or vectorization.** When building BoW vectors, we generally create a **features dictionary** of all vocabulary in our training data (usually several documents) mapped to indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Features Dictionary\n",
    "```python\n",
    "from preprocessing import preprocess_text\n",
    "# Define create_features_dictionary() below:\n",
    "def create_features_dictionary(documents):\n",
    "  features_dictionary = {}\n",
    "  merged = \" \".join(documents)\n",
    "  tokens = preprocess_text(merged)\n",
    "  index = 0\n",
    "  for token in tokens:\n",
    "    if token not in features_dictionary:\n",
    "      features_dictionary[token] = index\n",
    "      index += 1\n",
    "\n",
    "  return features_dictionary, tokens\n",
    "\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "\n",
    "print(create_features_dictionary(training_documents)[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a BoW Vector\n",
    "Each index in the list will correspond to a word and be set to its count.  \n",
    "\n",
    "```python\n",
    "from preprocessing import preprocess_text\n",
    "# Define text_to_bow_vector() below:\n",
    "def text_to_bow_vector(some_text, features_dictionary):\n",
    "  bow_vector = [0] * len(features_dictionary) # list of 0's length of dict\n",
    "  tokens = preprocess_text(some_text)\n",
    "  for token in tokens:\n",
    "    feature_index = features_dictionary[token]\n",
    "    bow_vector[feature_index] += 1\n",
    "    \n",
    "  return bow_vector, tokens\n",
    "\n",
    "\n",
    "features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}\n",
    "\n",
    "text = \"Another five fish find another faraway fish.\"\n",
    "print(text_to_bow_vector(text, features_dictionary)[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with most tasks in Python, there's already a library that can do all of the above work for you.  \n",
    "\n",
    "For `text_to_bow()`, you can approximate the functionality with the collections module’s `Counter()` function:\n",
    "```python\n",
    "from collections import Counter\n",
    " \n",
    "tokens = ['another', 'five', 'fish', 'find', 'another', 'faraway', 'fish']\n",
    "print(Counter(tokens))\n",
    " \n",
    "# Counter({'fish': 2, 'another': 2, 'find': 1, 'five': 1, 'faraway': 1})\n",
    "```\n",
    "\n",
    "For vectorization, you can use `CountVectorizer` from the machine learning library `scikit-learn`. You can use `fit()` to train the features dictionary and then `transform()` to transform text into a vector:\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "test_text = [\"Another five fish find another faraway fish.\"]\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_vectorizer.fit(training_documents)\n",
    "bow_vector = bow_vectorizer.transform(test_text)\n",
    "print(bow_vector.toarray())\n",
    "# [[2 0 1 1 2 1 0 0 0 0 0 0 0 0 0]]\n",
    "```\n",
    "\n",
    "#### script.py\n",
    "```python\n",
    "from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import CountVectorizer from sklearn:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define bow_vectorizer:\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# Define training_vectors:\n",
    "training_vectors = bow_vectorizer.fit_transform(training_docs)\n",
    "# Define test_vectors:\n",
    "test_vectors = bow_vectorizer.transform(test_docs)\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "\n",
    "def spam_or_not(label):\n",
    "  return \"spam\" if label else \"not spam\"\n",
    "\n",
    "# Uncomment the code below when you're done:\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "\n",
    "predictions = spam_classifier.score(test_vectors, test_labels)\n",
    "\n",
    "print(\"The predictions for the test data were {0}% accurate.\\n\\nFor example, '{1}' was classified as {2}.\\n\\nMeanwhile, '{3}' was classified as {4}.\".format(predictions * 100, test_docs[7], spam_or_not(test_labels[7]), test_docs[15], spam_or_not(test_labels[15])))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Bag-of-Words\n",
    "You made it! And you’ve learned plenty about the bag-of-words language model along the way:\n",
    "\n",
    "- Bag-of-words (BoW) — also referred to as the unigram model — is a statistical language model based on word count.\n",
    "- There are loads of real-world applications for BoW.\n",
    "- BoW can be implemented as a Python dictionary with each key set to a word and each value set to the number of times that word appears in a text.\n",
    "- For BoW, training data is the text that is used to build a BoW model.\n",
    "- BoW test data is the new text that is converted to a BoW vector using a trained features dictionary.\n",
    "- A feature vector is a numeric depiction of an item’s salient features.\n",
    "- Feature extraction (or vectorization) is the process of turning text into a BoW vector.\n",
    "- A features dictionary is a mapping of each unique word in the training data to a unique index. This is used to build out BoW vectors.\n",
    "- BoW has less data sparsity than other statistical models. It also suffers less from overfitting.\n",
    "- BoW has higher perplexity than other models, making it less ideal for language prediction.\n",
    "- One solution to overfitting is language smoothing, in which a bit of probability is taken from known words and allotted to unknown words.\n",
    "\n",
    "The spam data for this lesson were taken from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms%20spam%20collection).\n",
    "\n",
    "Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository http://archive.ics.uci.edu/ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spam_data.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a5966039c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspam_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_spam_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_doc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m test_text = \"\"\"\n",
      "\u001b[0;32m~/nltk-intro/spam_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_spam_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_doc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_spam_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_test_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_training_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spam_data.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_training_clean\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spam_data.p'"
     ]
    }
   ],
   "source": [
    "from spam_data import training_spam_docs, training_doc_tokens, training_labels, training_docs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "test_text = \"\"\"\n",
    "Play around with the spam classifier!\n",
    "\"\"\"\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "training_vectors = bow_vectorizer.fit_transform(training_docs)\n",
    "test_vectors = bow_vectorizer.transform([test_text])\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "\n",
    "predictions = spam_classifier.predict(test_vectors)\n",
    "\n",
    "print(\"Looks like a normal email!\" if predictions[0] == 0 else \"You've got spam!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
